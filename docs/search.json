[
  {
    "objectID": "aboutPage/about.html",
    "href": "aboutPage/about.html",
    "title": "Hi, Iâ€™m Josh Pearlson",
    "section": "",
    "text": "I currently work in finance in NYC creating and applying advanced analytics to problems in fundamental and systematic investing. Previously, I graduated from Washington University in St.Â Louis where I studied Computer Science and Finance. My main interests are modeling, machine learning, and capital markets. I love meeting and connecting with new people, so feel free to email me or LinkedIn message me!\n\n\n\n\nExperienceAwardsPapersEducationHobbies\n\n\nBlackRock Analyst | Summer 2025 - Current | New York, NY \nBlackRock Intern | Summer 2024 | New York, NY  &gt; Securities lending optimization\nWashington University Natural Language Processing Laboratory Research Lab Member | August 2022 - August 2024  &gt; Joined pre-ChatGPT. Conducted NLP research using HPC to train and evaluate LLMs, supporting several research projects (including my own: DPFM@ICLR 2024).\nMoodyâ€™s Analytics Software Engineering Intern | Summer 2023 | New York, NY &gt; Worked on the NewsEdge team (NewsEdge) delivering ultra-low-latency news data.\nPlayBook (Startup) Data Science Intern | Summer 2022 | Boston, MA\nHarvard Surgical Navigation and Robotics Laboratory Research Intern | Summer 2020 | Boston, MA\n\n\nOutstanding Junior Award | Department of Computer Science  Hackathon WashU 2023 Winner | Social Connections Category  Deanâ€™s List | All Semesters \n\n\nLarge Language Models Recognize Decentralized Finance Entities Lead Author - Data Problems for Foundation Models at International Conference on Learning Representations 2024 Link \nMapping Lesions from MP-MRI using Convolution Neural Network Lead Author - SPIE Medical Imaging 2020 Link\n\n\nWashington University in St.Â Louis BS in Computer Science & Finance August 2021 - May 2025\n\n\n\nPoker (PLO, NLTH, HU)\nChess\nBasketball (go Celtics)\nTennis\nGym (former Powerlifter for 3 years)"
  },
  {
    "objectID": "articles/blog.html",
    "href": "articles/blog.html",
    "title": "Articles",
    "section": "",
    "text": "The Future of Context, DeepSeek OCR\n\n\n\n\n\n\nMachine Learning\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 31, 2025\n\n\nJosh Pearlson\n\n\n\n\n\n\n\n\n\n\n\n\nModeling a Coin Flip Game\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\n\n\n\n\n\n\nAug 16, 2024\n\n\nJosh Pearlson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html",
    "title": "Modeling a Coin Flip Game",
    "section": "",
    "text": "A casino offers a game: Flip a coin as long as you want. Your prize is the ratio of heads to total flips. Whatâ€™s your strategy to maximize your winnings?"
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#coin-flip-game",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#coin-flip-game",
    "title": "Modeling a Coin Flip Game",
    "section": "Coin Flip Game",
    "text": "Coin Flip Game\nImagine youâ€™re in a casino, instead of slot machines or table games you are offered a coin game. He explains the rules:\n\nI will pay you based on the outcome of a coin flip game. You can flip the coin as many times as you want, and you can stop whenever you like. I will pay you based on the ratio of heads to total flips. I will charge you 75 cents to play and pay you your heads to total flips ratio in cents. Would you like to play?\n\nAt first glance it appears the casino is offering us a losing wager (as is typical). In any long-run scenario you would expect to get a ratio of 1/2 heads to total flips, since heads and tails are equally likely to occur on any given flip. In that case, we would be paying 75 cents to play a game where we would only be getting paid 50 (1 heads for every 1 tails) cents! While this may seem like a losing game at first, we will suspend our intuition and allow for logic to guide our decision making. In the end, we will learn about the value of optionality and how it is used to our advantage in this game."
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#basic-strategy",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#basic-strategy",
    "title": "Modeling a Coin Flip Game",
    "section": "Basic Strategy",
    "text": "Basic Strategy\nFirst, letâ€™s review the problem: We have a fair coin (50% heads, 50% tails) and we can flip it as many times as we want. Our goal is to maximize the ratio of heads to total flips.\nFirst, we should think about a simplified case to get a better understanding of the game dynamics. We will create a decision tree to visualize our options following the outcome of the first flip:\nStart Game\n     |\n     +-- Flip 1: Heads (50% chance)\n     |     |\n     |     +-- Ratio is 1.0 (1 head / 1 flip)\n     |     +-- DECISION: STOP. This is the best possible ratio.\n     |     +-- OUTCOME: Profit (1.0 payout &gt; 0.75 cost)\n     |\n     +-- Flip 1: Tails (50% chance)\n           |\n           +-- Ratio is 0.0 (0 heads / 1 flip)\n           +-- DECISION: CONTINUE. Stopping means a 0.0 payout.\n                 |\n                 +-- Keep flipping...\n                 +-- Long-run ratio approaches 0.5\n                 +-- OUTCOME: Loss (0.5 payout &lt; 0.75 cost)\nThe key thing to note here is that when we flip heads on the first flip we win the game outright with the best possible payout ratio of 1.0 heads/total flips (giving us the gameâ€™s maximum profit of 100 (payout) - 75 (cost) = 25 cents). However if we flip tails on the first flip we are in a losing situation where we will have to continue flipping, right now we will assume that we will just flip until we get the long-run ratio of 0.5 heads/total flips.\nAs you can see, based on the naive strategy I have outlined above we offer a lower bound on the expected value of this game itself. We have devised a strategy playing the game where half of the time we will win 25 cents (flipping heads on the first flip) and half of the time we will lose 25 cents (flipping tails on the first flip and continuing to the long-run ratio of 0.5 heads/total flips). Thus we can say that the expected value of this game is at least:\n\n\\[\n\\begin{aligned}\nEV_{game} &\\geq  \\\\\n&\\frac{1}{2} (1.0 - 0.75) +\n\\frac{1}{2} (0.5 - 0.75)  \\\\\n&= \\frac{1}{2} (0.25) +\n\\frac{1}{2} (-0.25) \\\\\n&= \\$0\n\\end{aligned}\n\\]\n\nIf we wish to improve on this strategy we need to consider a new approach. What if we consider stopping at the first time that we see we have flipped heads more often than tails?\n\nStop When Heads &gt; Tails Strategy\nI will create a table with the first couple of possible stopping points (locations where # heads &gt; # tails) for this game:\n\n\n\n\n\n\n\n\n\n\nFlip Sequence\nHeads\nTails\nRatio (Heads/Total Flips)\nProbability of sequence\n\n\n\n\nH\n1\n0\n1.0\n1/2\n\n\nT H H\n2\n1\n0.67\n1/2 ^3\n\n\nT T H H H\n3\n2\n0.60\n1/2 ^5\n\n\nT H T H H\n3\n2\n0.60\n1/2 ^5\n\n\n\nFrom the table above we can see that if we stop at the first time we have more heads than tails we can achieve a ratio of 1.0, .67, .60, etc. Each stopping point is less and less likely, there is \\(\\frac{1}{2}^{\\text{\\# total flips}}\\) chance of achieving each outcome. However, we can see that the ratios we achieve are all greater than 0.5 heads/total flips. Another thing we notice is that the longer sequences clearly have many more paths (at much lower likelihoods) than the shorter sequences.\nLetâ€™s plot the possible outcomes of this game to count the number of paths that lead to each outcome:\n\n\n\n\n\n\n\n\n\nAs we can see above, we have the blue line showing the threshold where the number of heads and tails flipped are equal. The red dots are showing us our stopping points, where we have flipped more heads than tails for the first time. Only valid paths from (0,0) to each red dot without crossing the blue line are possible outcomes of the strategy! We also know that each time we flip a coin N times our chance of getting one exact path is 1/2^N. Given this graphical representation we can now use combinatorics to count the number of paths that lead to each stopping point and multiply this by our probability of getting to that same path.\nFor some arbitrary number k, we will stop when we have flipped k+1 heads and k tails. The total number of flips will be 2k+1 and our heads/total flips ratio will be (k+1)/(2k+1).\nThe EV of this game can be calculated as the sum of each flips ratio multiplied by the probability of reaching that ratio minus the cost to play (75 cents).\n\n\\[\n\\begin{aligned}\nEV_{game} &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\text{Heads}}{\\text{Total Flips}} \\cdot P(k) \\right) - 0.75 \\\\\nEV_{game} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot P(k) \\right) - 0.75 \\\\\n\\end{aligned}\n\\]\n\nOk, So how do we calculate P(k)? Enter, our graphical representation above! Here we can see that there are a number of paths that lead to each stopping point. However not all paths are valid, since some paths cross the blue line (where heads = tails) before reaching the stopping point. To calculate the number of valid paths to each stopping point we can use the ballot theorem from combinatorics.\n\nThe ballot theorem states that if in an election candidate A receives h votes and candidate B receives k votes with h&gt;k, then the number of ways the votes can be counted such that A is always strictly ahead of B is given by:\n\n\n\\[\n\\begin{aligned}\n\\frac{h-k}{h+k} \\binom{h+k}{k} \\\\\n\\end{aligned}\n\\]\n\nA special case of this theorem is when h = k + 1, exceeding votes by exactly one. If we think back to path counting, this problem is equivalent to us counting paths where we stop when heads exceeds tails by exactly one! Letâ€™s now solve for this special case:\n\n\\[\n\\begin{aligned}\n% & \\frac{h-k}{h+k} \\binom{h+k}{k} \\\\\n& h = k + 1 \\\\\n& \\text{Substituting h for its new value of k + 1}  \\\\\n& = \\frac{(k+1)-k}{(k+1)+k} \\binom{(k+1)+k}{k} \\\\\n& = \\frac{1}{2k+1} \\binom{2k+1}{k} \\\\\n& = \\frac{1}{2k+1} \\cdot \\frac{(2k+1)!}{(k+1)! k!} \\\\\n& (2k+1)! = (2k+1)(2k!) \\\\\n& = \\frac{1}{2k+1} \\cdot \\frac{(2k+1)(2k!)}{(k+1)! k!} \\\\\n& = \\frac{2k!}{(k+1)! k!} \\\\\n& (k+1)! = (k+1)(k!) \\\\\n& = \\frac{2k!}{(k+1)(k!) k!} \\\\\n& = \\frac{1}{k+1} \\cdot \\frac{2k!}{k! k!} \\\\\n& = \\frac{1}{k+1} \\binom{2k}{k} \\\\\n\\end{aligned}\n\\]\n\nThe sequence we have arrived at above is a very famous sequence in combinatorics called the Catalan numbers. Catalan numbers have many applications in combinatorial mathematics including counting valid parentheses expressions, counting rooted binary trees, and counting paths in a grid that do not cross a diagonal line. In our case we are using them to count the number of valid paths to each stopping point in our coin flip game. The number of paths to the k-th number stopping point (where num heads = k + 1 and num tails = k) happens to be the k-th Catalan number!\nNow that we have everything we need, Letâ€™s calculate the expected value of the stop when heads exceeds tails by one strategy.\n\n\nCalculating EV of Stop When Heads &gt; Tails\nStrategy: Let each stopping point k be defined as the first time # heads = k + 1 and # tails = k. In this game we are paid out based on the number of heads / total flips ratio at some stopping point k. As defined previously the value at the k-th stopping is (k + 1) / (2k + 1). The probability of reaching this stopping point is equal to the number of valid paths to this stopping point (equivalent to the kth Catalan number) multiplied by the probability of reaching any one path (1/2^(total flips) = (1/2 ^ (2k+1)). We also know trivially that the EV of the game is equal to the payout minus the cost to play the game (75 cents). First letâ€™s calculate the expected payout (EP) of this strategy:\n\n\\[\n\\begin{aligned}\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\text{Heads}}{\\text{Total Flips}} \\cdot P(k) \\right) \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot P(k) \\right) \\\\\nP(k) &= C_k \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot C_k \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot \\frac{1}{k+1} \\binom{2k}{k} \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{1}{2k+1} \\cdot \\binom{2k}{k} \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\   \nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{1}{2k+1} \\cdot \\frac{(2k)!}{(k!)^2} \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\\n&\\text{After further simplification we arrive at:} \\\\\nEP_{strategy} &= \\frac{1}{2} * \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)} \\right) \\\\\n\\end{aligned}\n\\]\n\nHere we rely on a clever mathematical result that states that the sum above is equal to the Taylor series expansion of arcsin(x) evaluated at x = 1. The result is as follows:\n\n\\[\n\\begin{aligned}\n\\arcsin(x) &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)} x^{2k+1} \\right) \\\\\n\\arcsin(1) &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)}  \\right) \\\\\n\\arcsin(1) &= \\frac{\\pi}{2} \\\\\n\\frac{\\pi}{2} &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)} \\right)\\\\\nEP_{strategy} &= \\frac{1}{2} * \\frac{\\pi}{2} = \\frac{\\pi}{4} \\approx 0.7854 \\\\\nEV_{strategy} &= EP_{strategy} - \\text{Cost to Play} \\\\\nEV_{strategy} &= \\frac{\\pi}{4} - 0.75 \\approx 0.0354 \\\\\n\\end{aligned}\n\\]\n\nAmazing! We have now turned our break-even strategy into one that mathematically wins over time! By stopping the game at the first time we have flipped more heads than tails we can expect to win about 3.54 cents per game in the long-run. This would be a ROI of about 4.72% on each 75 cent wager!"
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#strategy-improvement",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#strategy-improvement",
    "title": "Modeling a Coin Flip Game",
    "section": "Strategy Improvement",
    "text": "Strategy Improvement\nThe â€˜stop when heads &gt; tailsâ€™ strategy clearly works, however, it seems like there could be a better strategy. What if we only accept a heads/total flips ratio of greater than 0.51, or 0.55, or 0.60? Could we do even better?\nIn this exploration two things interest me most:\n\nWhat is the optimal policy for the â€˜stop when heads/total flips &gt; Xâ€™ strategy?\nIs there a better policy that could outperform stop when â€˜heads/total flips &gt; Xâ€™?\n\nFirst, letâ€™s tackle modeling the â€˜stop when heads/total flips &gt; Xâ€™ strategy. I wrote a simple python simulation to model this game and test various stopping points. The code is as follows:\n\nPython Simulation for â€˜Stop When Heads/Total Flips &gt; Xâ€™ Strategy\n\ndef run_numpy_simulation(num_simulations: int, max_flips: int, stop_ratio: float) -&gt; float:\n    flips = np.random.randint(\n        0, # Inclusive lower bound\n        2, # Exclusive upper bound\n        size=(num_simulations, max_flips) # Number of simulations, number of flips per simulation\n        )\n\n    num_heads = np.cumsum(flips, axis=1) # Cumulative sum of heads along each row\n    flip_numbers = np.arange(1, max_flips + 1) # Total flips \n    ratios = num_heads / flip_numbers # Ratio of heads to total flips\n    exceeds_threshold = ratios &gt; stop_ratio # Find where ratios exceeded the stop ratio\n    first_exceed_indices = np.argmax(exceeds_threshold, axis=1) # Find the first time where this happened\n\n    # If we never exceed, set the index to last flip\n    never_exceeds = ~exceeds_threshold.any(axis=1)\n    first_exceed_indices[never_exceeds] = max_flips - 1\n\n    final_ratios = ratios[np.arange(num_simulations), first_exceed_indices] # Final ratios\n    final_ratios[never_exceeds] = np.maximum(0.5, final_ratios[never_exceeds]) # Ensure min ratio of 0.5 (long-run average)\n    return float(np.mean(final_ratios))\n\nprint(\"Running simulations...\")\nnum_simulations = 2_000_000  # 2 million sims \nmax_flips = 1000 # 1000 max flips per simulation\n\nresults = []\n# Test a range of stop_ratios from 0.50 to 0.64\nstop_ratios_to_test = np.arange(0.5, 0.64, 0.01)\n\nfor stop_ratio in stop_ratios_to_test:\n    print(f\"Simulating for stop ratio: {stop_ratio:.2f}...\")\n    avg_ratio = run_numpy_simulation(num_simulations, max_flips, stop_ratio)\n    results.append({'Stop Ratio': round(stop_ratio, 2), 'Expected Payout': avg_ratio})\n    print(\"=\"*50)\n\nprint(\"Simulations complete.\")\n# Results Dataframe! \ndf_all_results = pd.DataFrame(results).set_index('Stop Ratio')\n\n\ndf_all_results['Expected Value'] =  df_all_results['Expected Payout'] - 0.75  # Adjusted Expected Value\ndf_all_results['Expected Payout'].plot(\n    kind='line',\n    figsize=(10, 6),\n    marker='o',\n    title=f'Expected Payout across {num_simulations:,} Simulations',\n    xlabel='Stop Ratio',\n    ylabel='Heads / Total Flips',\n    color ='green'\n)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_all_results\n\n\n\n\n\n\n\n\nExpected Payout\nExpected Value\n\n\nStop Ratio\n\n\n\n\n\n\n0.50\n0.785382\n0.035382\n\n\n0.51\n0.785749\n0.035749\n\n\n0.52\n0.786531\n0.036531\n\n\n0.53\n0.787559\n0.037559\n\n\n0.54\n0.788118\n0.038118\n\n\n0.55\n0.788536\n0.038536\n\n\n0.56\n0.788972\n0.038972\n\n\n0.57\n0.788434\n0.038434\n\n\n0.58\n0.788622\n0.038622\n\n\n0.59\n0.787903\n0.037903\n\n\n0.60\n0.787989\n0.037989\n\n\n0.61\n0.788028\n0.038028\n\n\n0.62\n0.786756\n0.036756\n\n\n0.63\n0.785774\n0.035774\n\n\n0.64\n0.784747\n0.034747\n\n\n\n\n\n\n\nFirst, we can confirm that these results are reasonable. Our â€˜stop when heads &gt; tailsâ€™ strategy that we calculated earlier using pure mathematics is equivalent to a stop ratio of 0.5 heads / total flips. We can see that for 0.50 stop ratio our payout is modeled at about 0.7854, which is the exact value that we got when mathematically calculating its expected payout!\nFrom these results, it appears the optimal stopping ratio is around 0.56, yielding an expected payout of about 0.789 heads/total flips. This is approximately equal to a profit of 3.9 cents per game played! This is slightly better than our previous strategy of stopping when heads first exceeds tails. The 3.9 cent profit per game is now an ROI of about 5.2% on each 75 cent wager.\n\n\nThe Most Optimal Policy\nNow we have figured out the optimal policy for the â€˜stop when heads/total flips &gt; Xâ€™ strategy. However, is there a better policy that could outperform stop when â€˜heads/total flips &gt; Xâ€™?\nIn theory, yes. By creating some policy that scales stopping ratio by marginal variance per flip you could achieve better outcomes (think about this and try it out yourself). However, instead we will focus on a more theoretically interesting idea.\nBut first as an aside, letâ€™s introduce potentially one of the coolest named theorems in probability: the Infinite Monkey Theorem (link).\n\n\n\nInfinite Monkey Theorem\n\n\n\nThe core idea of the infinite monkey theorem is a thought experiment. Letâ€™s say you have one monkey and have him sit down and randomly hit keys on a typewriter for an infinite amount of time. Given infinite time, the monkey will certainly type out any given possible text, including the complete works of Shakespeare.\n\nWell, letâ€™s follow this logic for our coin flip game. Given infinite time, we would expect any sequence of heads and tails to eventually occur (if we never stop flipping). Obviously, in practice Python cannot model infinite time. However, a new policy appears when we consider this theorem. What if we simply only stop flipping when we reach a ratio of .999 repeated heads/total flips? While this may feel like a cop-out answer, it is in theory the optimal policy given we have infinite time to continue flipping the coin and never stop. If we truly can flip forever, or even simulate in some way infinite time, we would eventually reach a ratio of .999 repeated heads/total flips! This would give us a payout of 99.99 repeated cents, and a profit of 24.99 repeated cents per game played! This would be a massive ROI of 33.33 % repeated."
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#findings",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#findings",
    "title": "Modeling a Coin Flip Game",
    "section": "Findings",
    "text": "Findings\nThis analysis reveals that a seemingly unfavorable coin-flip game can be transformed into a profitable venture through the strategic application of optionality. The core finding is that by having the freedom to stop at any point, a player can devise a policy that systematically beats the gameâ€™s apparent long-run expectation of a 50-cent payout.\n\nAnalytical Solution:\n\nWe first established a baseline profitable strategy: stopping the game the first time the number of heads exceeds the number of tails. Using combinatorics and the properties of Catalan numbers, we analytically derived the expected payout for this strategy to be exactly Ï€/4 â‰ˆ 0.7854. With a cost of 0.75, this yields a positive expected value of approximately 3.54 cents per game.\n\nNumerical Optimization:\n\nThrough large-scale Monte Carlo simulations (2 million runs), we refined this approach by testing various static stopping thresholds. The simulations confirmed our analytical result and identified a more optimal policy: stopping only when the ratio of heads to total flips exceeds 0.56. This optimized policy increases the expected payout to approximately 0.789, boosting the expected profit to 3.9 cents per game and achieving a return on investment of over 5%.\n\nThe Value of Optionality:\n\nThe essential insight is that the playerâ€™s ability to choose when to stop provides a significant edge. While the long-term probability of heads is 50%, the player can selectively end the game during periods of favorable short-term variance, thereby capturing a payout greater than the underlying average."
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#future-work",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#future-work",
    "title": "Modeling a Coin Flip Game",
    "section": "Future Work",
    "text": "Future Work\nThe strategies explored in this article, while profitable, are based on a static stopping rule. This opens the door to several avenues for more advanced research and optimization.\n\nDynamic Stopping Policy:\n\nThe primary limitation of the current model is its use of a constant stopping ratio. A more sophisticated approach would be to implement a dynamic policy where the decision to stop or continue depends on the current state of the game (i.e., the number of heads and the total number of flips). For instance, a 52% ratio after 1,000 flips is far more statistically significant and secure than a 52% ratio after only 25 flips.\n\nAnalysis with a Biased Coin:\n\nThe analysis could be extended to scenarios involving a biased coin (where P(Heads) â‰  0.5). This would require recalculating the expected values and likely shift the optimal stopping thresholds, providing insight into how the strategy adapts to different underlying probabilities.\n\nRisk-Adjusted Strategies:\n\nThe current work focuses exclusively on maximizing expected value. A real-world player might be risk-averse and prefer a strategy that, for example, maximizes the probability of breaking even or minimizes the chance of a significant loss. Future analysis could incorporate utility theory to develop strategies that align with different risk profiles.\n\nExploring Different Payout Structures:\n\nHow would the optimal strategy change if the casino altered the payout function? Investigating non-linear payouts (e.g., (heads/total_flips)^2) or introducing different cost structures would test the robustness of our findings and lead to more generalized solutions for this class of optionality-based games.\n\nThese are just some initial thoughts by me and I am open to discussion. As always, feel free to reach out to me for suggestions on future articles or comments."
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html",
    "href": "projects/6CardGolf/IMPROVEMENTS.html",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Issue: flippedInitialCards, flippedOpponentInitialCards, flippedHostInitialCards were used but not initialized in gameState\nFix: Added these variables to the gameState object and properly reset them in resetGameState()\n\n\n\n\n\nIssue: Discard pile was sometimes treated as an array, sometimes as a single card\nFix: Ensured discard pile is always treated as an array with proper null checks\n\n\n\n\n\nIssue: Timing issues during the initial card flipping phase could cause desynchronization\nFix: Added better logging and validation to track flip progress and prevent race conditions\n\n\n\n\n\nIssue: WebRTC connection failures and message parsing errors werenâ€™t properly handled\nFix: Added comprehensive error handling with user-friendly messages\n\n\n\n\n\nIssue: Cards could become unclickable unexpectedly due to improper state management\nFix: Improved button state management and added validation checks\n\n\n\n\n\n\n\n\nAdded timeout mechanism for ICE gathering (10 seconds)\nImplemented heartbeat mechanism to detect connection issues\nAdded better error recovery and user feedback\nImproved connection state monitoring\n\n\n\n\n\nAdded validateGameState() function to catch inconsistencies early\nIntegrated validation into UI updates\nAdded safety checks for array operations\n\n\n\n\n\nAdded null checks for all array operations\nImproved error messages with context\nAdded system chat messages for connection issues\n\n\n\n\n\nAdded more responsive CSS for mobile devices\nImproved button state management\nBetter visual feedback for connection status\nAdded logging for debugging\n\n\n\n\n\nAdded comprehensive logging for debugging\nImproved function documentation\nBetter separation of concerns\nMore robust state management\n\n\n\n\n\n\n\n\nAutomatically detects connection issues\nSends periodic heartbeat messages every 30 seconds\nProvides early warning of connection problems\n\n\n\n\n\nBetter responsive design for small screens\nImproved touch targets and spacing\nOptimized layout for mobile devices\n\n\n\n\n\nAdded validation function to catch state inconsistencies\nEnhanced logging throughout the application\nBetter error reporting to users\n\n\n\n\n\n\n\n\nProper cleanup of intervals and event listeners\nBetter resource management for WebRTC connections\n\n\n\n\n\nReduced unnecessary UI updates\nBetter state synchronization\nImproved message handling efficiency\n\n\n\n\n\nBetter input validation\nSanitized message handling\nProtected against malformed data\n\n\n\n\n\n\nConnection Testing: Test with various network conditions\nMobile Testing: Verify responsive design on different screen sizes\nEdge Cases: Test with slow connections and connection drops\nGame Logic: Verify all game rules work correctly\nError Recovery: Test recovery from various error conditions\n\n\n\n\n\nReconnection Logic: Automatic reconnection when connection is lost\nGame History: Save and display game history\nSound Effects: Add audio feedback for game actions\nAnimations: Smooth transitions between game states\nAccessibility: Improve accessibility features\nOffline Mode: Local game mode for practice\n\n\n\n\n\nwebrtc.js: WebRTC connection improvements and heartbeat mechanism\n6cardgolf.js: Game logic fixes and validation\n6cardgolfUI.js: UI improvements and better state management\nstyle.css: Enhanced responsive design\nindex.html: No changes needed\n\nAll improvements maintain backward compatibility and donâ€™t break existing functionality."
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#critical-bugs-fixed",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#critical-bugs-fixed",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Issue: flippedInitialCards, flippedOpponentInitialCards, flippedHostInitialCards were used but not initialized in gameState\nFix: Added these variables to the gameState object and properly reset them in resetGameState()\n\n\n\n\n\nIssue: Discard pile was sometimes treated as an array, sometimes as a single card\nFix: Ensured discard pile is always treated as an array with proper null checks\n\n\n\n\n\nIssue: Timing issues during the initial card flipping phase could cause desynchronization\nFix: Added better logging and validation to track flip progress and prevent race conditions\n\n\n\n\n\nIssue: WebRTC connection failures and message parsing errors werenâ€™t properly handled\nFix: Added comprehensive error handling with user-friendly messages\n\n\n\n\n\nIssue: Cards could become unclickable unexpectedly due to improper state management\nFix: Improved button state management and added validation checks"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#improvements-made",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#improvements-made",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Added timeout mechanism for ICE gathering (10 seconds)\nImplemented heartbeat mechanism to detect connection issues\nAdded better error recovery and user feedback\nImproved connection state monitoring\n\n\n\n\n\nAdded validateGameState() function to catch inconsistencies early\nIntegrated validation into UI updates\nAdded safety checks for array operations\n\n\n\n\n\nAdded null checks for all array operations\nImproved error messages with context\nAdded system chat messages for connection issues\n\n\n\n\n\nAdded more responsive CSS for mobile devices\nImproved button state management\nBetter visual feedback for connection status\nAdded logging for debugging\n\n\n\n\n\nAdded comprehensive logging for debugging\nImproved function documentation\nBetter separation of concerns\nMore robust state management"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#new-features-added",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#new-features-added",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Automatically detects connection issues\nSends periodic heartbeat messages every 30 seconds\nProvides early warning of connection problems\n\n\n\n\n\nBetter responsive design for small screens\nImproved touch targets and spacing\nOptimized layout for mobile devices\n\n\n\n\n\nAdded validation function to catch state inconsistencies\nEnhanced logging throughout the application\nBetter error reporting to users"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#technical-improvements",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#technical-improvements",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Proper cleanup of intervals and event listeners\nBetter resource management for WebRTC connections\n\n\n\n\n\nReduced unnecessary UI updates\nBetter state synchronization\nImproved message handling efficiency\n\n\n\n\n\nBetter input validation\nSanitized message handling\nProtected against malformed data"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#testing-recommendations",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#testing-recommendations",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Connection Testing: Test with various network conditions\nMobile Testing: Verify responsive design on different screen sizes\nEdge Cases: Test with slow connections and connection drops\nGame Logic: Verify all game rules work correctly\nError Recovery: Test recovery from various error conditions"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#future-enhancements",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#future-enhancements",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Reconnection Logic: Automatic reconnection when connection is lost\nGame History: Save and display game history\nSound Effects: Add audio feedback for game actions\nAnimations: Smooth transitions between game states\nAccessibility: Improve accessibility features\nOffline Mode: Local game mode for practice"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#files-modified",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#files-modified",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "webrtc.js: WebRTC connection improvements and heartbeat mechanism\n6cardgolf.js: Game logic fixes and validation\n6cardgolfUI.js: UI improvements and better state management\nstyle.css: Enhanced responsive design\nindex.html: No changes needed\n\nAll improvements maintain backward compatibility and donâ€™t break existing functionality."
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "6 Card Golf \n       P2P 1v1 card game with a chat. \n    \n  \n\n  \n    \n      Â \n      \n       Dig Dug Game \n      A classic arcade game recreation."
  },
  {
    "objectID": "articles/posts/deepseek-ocr/deepseek-ocr.html",
    "href": "articles/posts/deepseek-ocr/deepseek-ocr.html",
    "title": "The Future of Context, DeepSeek OCR",
    "section": "",
    "text": "A breakthrough in optical character recognition (OCR) has profound implications for the future of LLMs.\nLarge language models are usually limited by how much text they can fit into their context windows. Every token matters. This is especially true when processing long PDFs, research papers, or codebases. But a recent breakthrough in optical character recognition (OCR) suggests something surprising: vision tokens may soon become a dramatically more efficient way to feed information to LLMs than text tokens.\nIn this article, Iâ€™ll explain what OCR actually is, how the technology evolved, what changed recently, and why this discovery might reshape the future of context."
  },
  {
    "objectID": "articles/posts/deepseek-ocr/deepseek-ocr.html#what-the-hell-is-an-ocr",
    "href": "articles/posts/deepseek-ocr/deepseek-ocr.html#what-the-hell-is-an-ocr",
    "title": "The Future of Context, DeepSeek OCR",
    "section": "What the hell is an OCR?",
    "text": "What the hell is an OCR?\nOCR stands for Optical Character Recognition. It is the core technology that allows computers to convert real world documents, such as pdfs, images, and scanned paper documents, into machine-readable text. In other words, OCR transforms a messy visual input into a structured text representation that can be fed into other processes like LLMs.\n\nA Brief History of OCR\nOCR technology is not new, its history actually dates back to the 20th century. In the 1910s, Dr.Â Edmund Fournier dâ€™Albe of Birmingham University developed the first ever OCR system, named the Optophone.\n\n\n\nOptophone machine\n\n\nThe Optophone was able to scan printed text and convert it into distinct sounds allowing for a blind user to recognize what characters were being scanned. The system was rule-based and relied on the physical height and width of characters to identify them.\n\n\n\nOptophone Methodology\n\n\nNot much of significance happened with the technology until the 1950s, when developments in digital computers and light sensors allowed OCR to take off. OCR was no longer physical rules based systems and now involved using computers to run similarity algorithms evaluating which letters were present. IBM and RCA began to develop OCR technology for all sorts of tasks. IBM created the first commercially available scanner that could read handwritten text.\nIn the â€™60s and â€™70s, OCR became a key software for everything from the postal service sorting mail to banks processing checks (see MICR). Even to this day OCR software is used all around the world in a vast array of commercial applications.\nModern developments in the 21st century involving neural networks have offered even more room for OCR improvement, as we will see with DeepSeekOCR."
  },
  {
    "objectID": "articles/posts/deepseek-ocr/deepseek-ocr.html#deepseek-ocr",
    "href": "articles/posts/deepseek-ocr/deepseek-ocr.html#deepseek-ocr",
    "title": "The Future of Context, DeepSeek OCR",
    "section": "DeepSeek OCR",
    "text": "DeepSeek OCR\nEarlier this month, a Chinese hedge fundâ€™s AI lab, DeepSeek released a paper outlining a new OCR model. Without even reading the paper, the initial community reaction was something along the lines of:\n\nDeepSeek continues to enter a new market (OCR) and is killing it again (claiming 97%+ accuracy on general OCR benchmarks)\n\nOCR software is old, boring software that just turns writing/images into well formatted text and has many commercial applications for DeepSeek to monetize\n\nI would say the initial read and public reception of this work is 100% correct. However, it misses the whole point of the paper! There is one core aspect which outlines why DeepSeek went through the trouble to make yet another OCR model.\n\nItâ€™s always been about context!\nIn the world of machine learning and natural language processing, context is king. The ability to understand and utilize context is what allows models to generate coherent and relevant responses.\nAnyone who uses AI assistants learns quite quickly about the importance of context. Nothing is worse than chatting with the model for 10 minutes then having to start all over explaining a topic when you hit the context limit for the model. Eventually, you are forced to open a new ChatGPT or Claude tab and start the whole conversation all over again.\nHere is the one fascinating thing I have not yet told you about the brand new DeepSeek OCR model: vision tokens are dramatically more information-dense than text tokens when representing documents.\nNow this is an interesting development! Currently the largest LLM with strong performance and long context is the Gemini model, claiming 2 million tokens of context. Each token is approximately \\(\\frac{1}{4}\\) of a word. Given around 500 words per page, 2 million tokens of context gets you about 3,000 pages of context or about 1,500,000 words.\nIn this paper, DeepSeek is stating that they can compress text tokens 10x when using them as vision tokens with very high accuracy. If this is true you could extend Geminiâ€™s context window from 2 million tokens to 20 million. 1.5 million words versus 15 million words, 3,000 pages versus 30,000 pages. This is like going from being able to see 1 volume of the Encyclopedia Britannica versus 12 volumes (the full 32 volumes are 50 Million words).\n\n\nWhatâ€™s the catch?\nWell, I did say if this is true and currently it is not. Some language models understand vision tokens at high fidelity; however, this fidelity is not nearly on par with typical text. In order to achieve a full 10x of the context window the model would have to understand vision tokens just as well as it understands text tokens without decoding them back into text tokens (removing any compression benefit). Text has semantic meaning, typically previous words in a sentence have strong correlation with the future words in that same sentence. Vision tokens share this relationship but on a much smaller scale. Since the correlation to future tokens is so small, this leads to a much worse ability to generate coherent output.\nThere have been very interesting workarounds though and research in the space is ongoing. One pervasive thought in the industry right now is the idea of storing context in cached vision tokens, but only converting necessary context into text tokens for the model to actively use. This seems to be a plausible and reasonable future development of vision token compression.\n\n\nWell, letâ€™s test it out!\nLast week the following was trending on both Twitter and Linkedin in relation to the new DeepSeek-OCR model.\n\n\n\nDeepSeek-OCR on Ramanujanâ€™s letter to GH Hardy 1913 (could not find any citation for this)\n\n\nThis looks shocking; the accuracy level seems to be very high even on complex handwritten mathematical formulas from over 100 years ago! This level of accuracy prompted (awful pun) a healthy level of skepticism. Letâ€™s test this ourselves and see how accurate it is on both the Ramanujan letter as well as some brand new handwriting of my own.\n\nDeepSeek-OCR result on letter to GH Hardy 1913\nThe following is the letter after being passed through DeepSeek-OCR at the highest compute setting, I used Google Colab with an A100 for this:\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ start â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“\nDear Mr Hardy,\nIn one of my letters I wrote about the least number of terms which will give the mean est integral to the actual coefficient in \\(\\frac{1}{2}\\) problem. It will be actually difficult to prove such a result. But we can prove this much as follows.\nequation:\n\\[\n\\begin{align*}\n\\sum a_n x^n &= \\frac{1}{1-50(1-\\frac{1}{2}x+\\frac{1}{1-2x}+\\cdots)} \\\\\na_n &= c \\left[ e^{2n\\pi} + (-1)^n \\frac{e^{n\\pi}}{2^5} + 2 \\cos\\left(\\frac{2n\\pi}{5} + 8(2n-1)\\frac{e^{\\frac{2n\\pi}{5}}}{5} + \\cdots\\right) + 2 \\cos\\left(\\frac{2n\\pi}{5} + 8(3n-1)\\frac{e^{\\frac{2n\\pi}{5}}}{5} + \\frac{2n\\pi}{5} + \\cdots\\right) \\right]\n\\end{align*}\n\\]\nequation:\n\\[\n\\begin{align*}\nc = \\frac{3}{2} \\cdot \\frac{[\\Gamma(\\frac{3}{2})]^6}{\\pi^6} = .94373...\n\\end{align*}\n\\]\ntext:\nWe shall first prove that, if we take only\n\\[\n\\begin{align*}\n\\left[ \\frac{3}{2} n(1-c) \\right]^{\\frac{1}{2}} \\sqrt{(1-\\frac{1}{2}n)(1-\\frac{1}{4}n)(1-\\frac{1}{8}n)\\cdots}\n\\end{align*}\n\\]\ntext:\nterms in the right-hand side of (1), E being any positive number less than 1 and 5, 13, 17, â€¦ are primes of the form \\(4k+1\\), then it is possible to find an infinity of values of \\(n\\) for which \\(dn\\) is not the mean of in- tegers to the sum of the asymptote series up to (2) terms. Suppose that \\(a_n\\) is the number of terms of any two squares such as 1, 2, 4, 5, 9, 10, 13, 16, â€¦ not exceeding \\(\\lambda\\) and that \\(p(\\lambda)\\) is the number of sums of two squares that are prime to each other such as 1, 2, 5, 10, 13, 17, 25 â€¦ not exceeding \\(\\lambda\\). Then it is easy to see that\n\nSpecial thank you to Trinity College for hosting this letter\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ end â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“\nWow, that is some shocking levels of accuracy. This level of accuracy on handwritten text seems insane. Just to confirm this is not already seen in the training data of the model I will now run the model but with some awful handwriting of my own. This is a handwritten page of homework from college. Similar to the page above it contains a lot of scribbled math which would be hard for even me to decipher at times.\nHere is the homework page:\n\n\n\nOld math homework\n\n\nHere is the model output:\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ start â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“\ntext:\nmiddle term is zero. Since \\(E_{\\epsilon}[\\epsilon] = 0\\)\nequation:\n\\[\n\\begin{align*}\nE_x[(g_D - s(x))^{2}] + \\sigma^{2}, \\qquad\nE_D!\\left[(g_D - s(x))^{2}\\right]\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n= \\left(E_D[g_D] - s(x)\\right)^{2} + \\mathrm{Var}_D(g_D) \\\nE\\left[E_D(g_D)\\right]\n= E_x!\\left[\\text{bias}^{2} + \\text{variance}\\right] + \\sigma^{2} \\\n= \\text{bias}^{2} + \\text{variance} + \\sigma^{2}\n\\end{align*}\n\\]\ntext:\n\\[\nE\\left[E_D(g_D)\\right]\n= E_x!\\left[\\text{bias}^{2} + \\text{variance}\\right] + \\sigma^{1}\n= \\text{bias}^{2} + \\text{variance} + \\sigma^{1}\n\\]\ntext:\n\n\n\nequation:\n\\[\n[\nD = {(x_1, x_1^2), (x_2, x_2^2)}, \\qquad\nP = 1 = \\mathrm{Dim}(D), \\qquad\ns(x) = x^{2}\n]\n\\]\ntext:\nunion over \\([-1, 1]\\), \\(H = \\mathrm{th}(h(x)) = ax + b\\) for some \\(a,b \\in \\mathbb{R}\\) and we wish to minimize Square Error!\ntext:\n\n\\(a x_1 + b = x_1^2,\\quad a x_2 + b = x_2^2\\)\n\nequation:\n\\[\n\\begin{align*}\na x_1 + b - x_1^{2} &= a x_2 + b - x_2^{2} \\\na x_1 - x_1^{2} &= a x_2 - x_2^{2} \\\na(x_1 - x_2) &= x_1^{2} - x_2^{2} \\\na(x_1 - x_2) &= (x_1 + x_2)(x_1 - x_2) \\\na &= x_1 + x_2\n\\end{align*}\n\\]\nequation:\n\\[\n\\begin{align*}\n(x_1 + x_2)x_1 + b &= x_1^{2} \\\nx_1^{2} + x_1 x_2 + b &= x_2^{2} \\\nb &= -x_1 x_2\n\\end{align*}\n\\]\nequation:\n\\[\n[\n\\tilde{g}(x)\n= E[h_0(x)]\n= E[(x_1 + x_2)x - x_1 x_2]\n= 0\n]\n\\]\ntext:\n\nTo numerically estimate \\(g(x)\\), \\(E_D(x)\\), bias, and variance, I have devised the following estimator. First, generate many training sets \\(g(x_1, x_2)\\) for values in uniform \\([-1,1]\\) space. Next we fit our hypothesis line to this data. The average of these hypothesis lines will be our estimate of \\(g(x)\\).\n\ntext:\nFor each hypothesis we also compute\n\\[\n[\nE_D(x) = \\frac{1}{n} \\sum_{i=1}^{n} (g(x_i) - x_i^{2})\n]\n\\]\ntext:\nFor every sample we solve this as follows: $ (x) = (g(x) - x)^2 $, where \\(g(x)\\) is always assumed.\ntext:\nOur variance estimator will be:\n\\[\n[\n\\mathrm{Var}(x)\n= \\frac{1}{n} \\sum_{i=1}^{n} \\left(g(x_i) - \\tilde{g}(x_i)\\right)^{2}.\n]\n\\]\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“ end â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“\nFor sure a lot more errors in this one in terms of OCR quality. On my handwriting the model frequently messes up what the power was of variables, exact forms of equations, confuses vectors, and more. Given this performance I would highly agree with the comments I have seen on social media that the letters from Ramanujan to GH Hardy were likely in the training set of the DeepSeekOCR model. Performance on my handwriting was still very strong, but clearly not to the caliber of a model that almost perfectly gets the scribbled handwriting from the Ramanujan letter.\n\n\n\nCurrent use cases\nNaively, as a recent graduate who frequently had to write assignments in LaTeX (a mathematical formatting language) which was often time consuming given most of the math was worked out on paper (or iPad) already, I see some great use cases.\n\nPass paper homework into DeepSeekOCR, convert to markdown.\nAsk LLM to convert markdown to LaTeX\nVoila! You have saved ~an hour of writing your already-written homework into LaTeX!\n\nRealistically this is not the true value of this model, but nonetheless a very interesting use case. The real value of this model is in converting large quantities of handwritten text into visual tokens as a medium (of high information density and low loss), instead of converting them directly into text tokens. If breakthroughs happen in how visual tokens can be used to train models directly, this paper will become seen as a seminal work in token density. Honestly, on a personal note, I hope there is more development in this space; the medium of visual tokens has much more to offer than asking SORA to generate you a video of Trump doing a backflip onto the White House lawn.\nAs always, till next time.\nJCP"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JCP",
    "section": "",
    "text": "jcp@home:~\n\n\n\n\n\n\n\n\n ðŸŒŒ"
  }
]
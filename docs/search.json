[
  {
    "objectID": "aboutPage/about.html",
    "href": "aboutPage/about.html",
    "title": "Hi, I’m Josh Pearlson",
    "section": "",
    "text": "I currently work in finance in NYC creating and applying advanced analytics to problems in fundamental and systematic investing. Previously, I graduated from Washington University in St. Louis where I studied Computer Science and Finance. My main interests are modeling, machine learning, and capital markets. I love meeting and connecting with new people, so feel free to email me or LinkedIn message me!\n\n\n\n\nExperienceAwardsPapersEducationHobbies\n\n\nBlackRock Analyst | Summer 2025 - Current | New York, NY \nBlackRock Intern | Summer 2024 | New York, NY  &gt; Securities lending optimization\nWashington University Natural Language Processing Laboratory Research Lab Member | August 2022 - August 2024  &gt; Joined pre-ChatGPT. Conducted NLP research using HPC to train and evaluate LLMs, supporting several research projects (including my own: DPFM@ICLR 2024).\nMoody’s Analytics Software Engineering Intern | Summer 2023 | New York, NY &gt; Worked on the NewsEdge team (NewsEdge) delivering ultra-low-latency news data.\nPlayBook (Startup) Data Science Intern | Summer 2022 | Boston, MA\nHarvard Surgical Navigation and Robotics Laboratory Research Intern | Summer 2020 | Boston, MA\n\n\nOutstanding Junior Award | Department of Computer Science  Hackathon WashU 2023 Winner | Social Connections Category  Dean’s List | All Semesters \n\n\nLarge Language Models Recognize Decentralized Finance Entities Lead Author - Data Problems for Foundation Models at International Conference on Learning Representations 2024 Link \nMapping Lesions from MP-MRI using Convolution Neural Network Lead Author - SPIE Medical Imaging 2020 Link\n\n\nWashington University in St. Louis BS in Computer Science & Finance August 2021 - May 2025\n\n\n\nPoker (PLO, NLTH, HU)\nChess\nBasketball (go Celtics)\nTennis\nGym (former Powerlifter for 3 years)"
  },
  {
    "objectID": "articles/blog.html",
    "href": "articles/blog.html",
    "title": "Priors",
    "section": "",
    "text": "The Six-Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”\n\n\n\n\n\n\nLLM\n\n\n\n\n\n\n\n\n\nDec 19, 2025\n\n\nJosh Pearlson\n\n\n\n\n\n\n\n\n\n\n\n\nGetting a Mortgage from the Market\n\n\n\n\n\n\nFinance\n\n\nOptions\n\n\n\n\n\n\n\n\n\nNov 22, 2025\n\n\nJosh Pearlson\n\n\n\n\n\n\n\n\n\n\n\n\nThe Future of Context, DeepSeek OCR\n\n\n\n\n\n\nMachine Learning\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 31, 2025\n\n\nJosh Pearlson\n\n\n\n\n\n\n\n\n\n\n\n\nModeling a Coin Flip Game\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\n\n\n\n\n\n\nAug 16, 2024\n\n\nJosh Pearlson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html",
    "title": "Modeling a Coin Flip Game",
    "section": "",
    "text": "A casino offers a game: Flip a coin as long as you want. Your prize is the ratio of heads to total flips. What’s your strategy to maximize your winnings?"
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#coin-flip-game",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#coin-flip-game",
    "title": "Modeling a Coin Flip Game",
    "section": "Coin Flip Game",
    "text": "Coin Flip Game\nImagine you’re in a casino, instead of slot machines or table games you are offered a coin game. He explains the rules:\n\nI will pay you based on the outcome of a coin flip game. You can flip the coin as many times as you want, and you can stop whenever you like. I will pay you based on the ratio of heads to total flips. I will charge you 75 cents to play and pay you your heads to total flips ratio in cents. Would you like to play?\n\nAt first glance it appears the casino is offering us a losing wager (as is typical). In any long-run scenario you would expect to get a ratio of 1/2 heads to total flips, since heads and tails are equally likely to occur on any given flip. In that case, we would be paying 75 cents to play a game where we would only be getting paid 50 (1 heads for every 1 tails) cents! While this may seem like a losing game at first, we will suspend our intuition and allow for logic to guide our decision making. In the end, we will learn about the value of optionality and how it is used to our advantage in this game."
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#basic-strategy",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#basic-strategy",
    "title": "Modeling a Coin Flip Game",
    "section": "Basic Strategy",
    "text": "Basic Strategy\nFirst, let’s review the problem: We have a fair coin (50% heads, 50% tails) and we can flip it as many times as we want. Our goal is to maximize the ratio of heads to total flips.\nFirst, we should think about a simplified case to get a better understanding of the game dynamics. We will create a decision tree to visualize our options following the outcome of the first flip:\nStart Game\n     |\n     +-- Flip 1: Heads (50% chance)\n     |     |\n     |     +-- Ratio is 1.0 (1 head / 1 flip)\n     |     +-- DECISION: STOP. This is the best possible ratio.\n     |     +-- OUTCOME: Profit (1.0 payout &gt; 0.75 cost)\n     |\n     +-- Flip 1: Tails (50% chance)\n           |\n           +-- Ratio is 0.0 (0 heads / 1 flip)\n           +-- DECISION: CONTINUE. Stopping means a 0.0 payout.\n                 |\n                 +-- Keep flipping...\n                 +-- Long-run ratio approaches 0.5\n                 +-- OUTCOME: Loss (0.5 payout &lt; 0.75 cost)\nThe key thing to note here is that when we flip heads on the first flip we win the game outright with the best possible payout ratio of 1.0 heads/total flips (giving us the game’s maximum profit of 100 (payout) - 75 (cost) = 25 cents). However if we flip tails on the first flip we are in a losing situation where we will have to continue flipping, right now we will assume that we will just flip until we get the long-run ratio of 0.5 heads/total flips.\nAs you can see, based on the naive strategy I have outlined above we offer a lower bound on the expected value of this game itself. We have devised a strategy playing the game where half of the time we will win 25 cents (flipping heads on the first flip) and half of the time we will lose 25 cents (flipping tails on the first flip and continuing to the long-run ratio of 0.5 heads/total flips). Thus we can say that the expected value of this game is at least:\n\n\\[\n\\begin{aligned}\nEV_{game} &\\geq  \\\\\n&\\frac{1}{2} (1.0 - 0.75) +\n\\frac{1}{2} (0.5 - 0.75)  \\\\\n&= \\frac{1}{2} (0.25) +\n\\frac{1}{2} (-0.25) \\\\\n&= \\$0\n\\end{aligned}\n\\]\n\nIf we wish to improve on this strategy we need to consider a new approach. What if we consider stopping at the first time that we see we have flipped heads more often than tails?\n\nStop When Heads &gt; Tails Strategy\nI will create a table with the first couple of possible stopping points (locations where # heads &gt; # tails) for this game:\n\n\n\n\n\n\n\n\n\n\nFlip Sequence\nHeads\nTails\nRatio (Heads/Total Flips)\nProbability of sequence\n\n\n\n\nH\n1\n0\n1.0\n1/2\n\n\nT H H\n2\n1\n0.67\n1/2 ^3\n\n\nT T H H H\n3\n2\n0.60\n1/2 ^5\n\n\nT H T H H\n3\n2\n0.60\n1/2 ^5\n\n\n\nFrom the table above we can see that if we stop at the first time we have more heads than tails we can achieve a ratio of 1.0, .67, .60, etc. Each stopping point is less and less likely, there is \\(\\frac{1}{2}^{\\text{\\# total flips}}\\) chance of achieving each outcome. However, we can see that the ratios we achieve are all greater than 0.5 heads/total flips. Another thing we notice is that the longer sequences clearly have many more paths (at much lower likelihoods) than the shorter sequences.\nLet’s plot the possible outcomes of this game to count the number of paths that lead to each outcome:\n\n\n\n\n\n\n\n\n\nAs we can see above, we have the blue line showing the threshold where the number of heads and tails flipped are equal. The red dots are showing us our stopping points, where we have flipped more heads than tails for the first time. Only valid paths from (0,0) to each red dot without crossing the blue line are possible outcomes of the strategy! We also know that each time we flip a coin N times our chance of getting one exact path is 1/2^N. Given this graphical representation we can now use combinatorics to count the number of paths that lead to each stopping point and multiply this by our probability of getting to that same path.\nFor some arbitrary number k, we will stop when we have flipped k+1 heads and k tails. The total number of flips will be 2k+1 and our heads/total flips ratio will be (k+1)/(2k+1).\nThe EV of this game can be calculated as the sum of each flips ratio multiplied by the probability of reaching that ratio minus the cost to play (75 cents).\n\n\\[\n\\begin{aligned}\nEV_{game} &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\text{Heads}}{\\text{Total Flips}} \\cdot P(k) \\right) - 0.75 \\\\\nEV_{game} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot P(k) \\right) - 0.75 \\\\\n\\end{aligned}\n\\]\n\nOk, So how do we calculate P(k)? Enter, our graphical representation above! Here we can see that there are a number of paths that lead to each stopping point. However not all paths are valid, since some paths cross the blue line (where heads = tails) before reaching the stopping point. To calculate the number of valid paths to each stopping point we can use the ballot theorem from combinatorics.\n\nThe ballot theorem states that if in an election candidate A receives h votes and candidate B receives k votes with h&gt;k, then the number of ways the votes can be counted such that A is always strictly ahead of B is given by:\n\n\n\\[\n\\begin{aligned}\n\\frac{h-k}{h+k} \\binom{h+k}{k} \\\\\n\\end{aligned}\n\\]\n\nA special case of this theorem is when h = k + 1, exceeding votes by exactly one. If we think back to path counting, this problem is equivalent to us counting paths where we stop when heads exceeds tails by exactly one! Let’s now solve for this special case:\n\n\\[\n\\begin{aligned}\n% & \\frac{h-k}{h+k} \\binom{h+k}{k} \\\\\n& h = k + 1 \\\\\n& \\text{Substituting h for its new value of k + 1}  \\\\\n& = \\frac{(k+1)-k}{(k+1)+k} \\binom{(k+1)+k}{k} \\\\\n& = \\frac{1}{2k+1} \\binom{2k+1}{k} \\\\\n& = \\frac{1}{2k+1} \\cdot \\frac{(2k+1)!}{(k+1)! k!} \\\\\n& (2k+1)! = (2k+1)(2k!) \\\\\n& = \\frac{1}{2k+1} \\cdot \\frac{(2k+1)(2k!)}{(k+1)! k!} \\\\\n& = \\frac{2k!}{(k+1)! k!} \\\\\n& (k+1)! = (k+1)(k!) \\\\\n& = \\frac{2k!}{(k+1)(k!) k!} \\\\\n& = \\frac{1}{k+1} \\cdot \\frac{2k!}{k! k!} \\\\\n& = \\frac{1}{k+1} \\binom{2k}{k} \\\\\n\\end{aligned}\n\\]\n\nThe sequence we have arrived at above is a very famous sequence in combinatorics called the Catalan numbers. Catalan numbers have many applications in combinatorial mathematics including counting valid parentheses expressions, counting rooted binary trees, and counting paths in a grid that do not cross a diagonal line. In our case we are using them to count the number of valid paths to each stopping point in our coin flip game. The number of paths to the k-th number stopping point (where num heads = k + 1 and num tails = k) happens to be the k-th Catalan number!\nNow that we have everything we need, Let’s calculate the expected value of the stop when heads exceeds tails by one strategy.\n\n\nCalculating EV of Stop When Heads &gt; Tails\nStrategy: Let each stopping point k be defined as the first time # heads = k + 1 and # tails = k. In this game we are paid out based on the number of heads / total flips ratio at some stopping point k. As defined previously the value at the k-th stopping is (k + 1) / (2k + 1). The probability of reaching this stopping point is equal to the number of valid paths to this stopping point (equivalent to the kth Catalan number) multiplied by the probability of reaching any one path (1/2^(total flips) = (1/2 ^ (2k+1)). We also know trivially that the EV of the game is equal to the payout minus the cost to play the game (75 cents). First let’s calculate the expected payout (EP) of this strategy:\n\n\\[\n\\begin{aligned}\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\text{Heads}}{\\text{Total Flips}} \\cdot P(k) \\right) \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot P(k) \\right) \\\\\nP(k) &= C_k \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot C_k \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{k+1}{2k+1} \\cdot \\frac{1}{k+1} \\binom{2k}{k} \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\\nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{1}{2k+1} \\cdot \\binom{2k}{k} \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\   \nEP_{strategy} &= \\sum_{k=0}^{\\infty} \\left( \\frac{1}{2k+1} \\cdot \\frac{(2k)!}{(k!)^2} \\cdot \\left( \\frac{1}{2} \\right)^{2k+1} \\right) \\\\\n&\\text{After further simplification we arrive at:} \\\\\nEP_{strategy} &= \\frac{1}{2} * \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)} \\right) \\\\\n\\end{aligned}\n\\]\n\nHere we rely on a clever mathematical result that states that the sum above is equal to the Taylor series expansion of arcsin(x) evaluated at x = 1. The result is as follows:\n\n\\[\n\\begin{aligned}\n\\arcsin(x) &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)} x^{2k+1} \\right) \\\\\n\\arcsin(1) &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)}  \\right) \\\\\n\\arcsin(1) &= \\frac{\\pi}{2} \\\\\n\\frac{\\pi}{2} &= \\sum_{k=0}^{\\infty} \\left( \\frac{\\binom{2k}{k}}{4^k (2k+1)} \\right)\\\\\nEP_{strategy} &= \\frac{1}{2} * \\frac{\\pi}{2} = \\frac{\\pi}{4} \\approx 0.7854 \\\\\nEV_{strategy} &= EP_{strategy} - \\text{Cost to Play} \\\\\nEV_{strategy} &= \\frac{\\pi}{4} - 0.75 \\approx 0.0354 \\\\\n\\end{aligned}\n\\]\n\nAmazing! We have now turned our break-even strategy into one that mathematically wins over time! By stopping the game at the first time we have flipped more heads than tails we can expect to win about 3.54 cents per game in the long-run. This would be a ROI of about 4.72% on each 75 cent wager!"
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#strategy-improvement",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#strategy-improvement",
    "title": "Modeling a Coin Flip Game",
    "section": "Strategy Improvement",
    "text": "Strategy Improvement\nThe ‘stop when heads &gt; tails’ strategy clearly works, however, it seems like there could be a better strategy. What if we only accept a heads/total flips ratio of greater than 0.51, or 0.55, or 0.60? Could we do even better?\nIn this exploration two things interest me most:\n\nWhat is the optimal policy for the ‘stop when heads/total flips &gt; X’ strategy?\nIs there a better policy that could outperform stop when ‘heads/total flips &gt; X’?\n\nFirst, let’s tackle modeling the ‘stop when heads/total flips &gt; X’ strategy. I wrote a simple python simulation to model this game and test various stopping points. The code is as follows:\n\nPython Simulation for ‘Stop When Heads/Total Flips &gt; X’ Strategy\n\ndef run_numpy_simulation(num_simulations: int, max_flips: int, stop_ratio: float) -&gt; float:\n    flips = np.random.randint(\n        0, # Inclusive lower bound\n        2, # Exclusive upper bound\n        size=(num_simulations, max_flips) # Number of simulations, number of flips per simulation\n        )\n\n    num_heads = np.cumsum(flips, axis=1) # Cumulative sum of heads along each row\n    flip_numbers = np.arange(1, max_flips + 1) # Total flips \n    ratios = num_heads / flip_numbers # Ratio of heads to total flips\n    exceeds_threshold = ratios &gt; stop_ratio # Find where ratios exceeded the stop ratio\n    first_exceed_indices = np.argmax(exceeds_threshold, axis=1) # Find the first time where this happened\n\n    # If we never exceed, set the index to last flip\n    never_exceeds = ~exceeds_threshold.any(axis=1)\n    first_exceed_indices[never_exceeds] = max_flips - 1\n\n    final_ratios = ratios[np.arange(num_simulations), first_exceed_indices] # Final ratios\n    final_ratios[never_exceeds] = np.maximum(0.5, final_ratios[never_exceeds]) # Ensure min ratio of 0.5 (long-run average)\n    return float(np.mean(final_ratios))\n\nprint(\"Running simulations...\")\nnum_simulations = 2_000_000  # 2 million sims \nmax_flips = 1000 # 1000 max flips per simulation\n\nresults = []\n# Test a range of stop_ratios from 0.50 to 0.64\nstop_ratios_to_test = np.arange(0.5, 0.64, 0.01)\n\nfor stop_ratio in stop_ratios_to_test:\n    print(f\"Simulating for stop ratio: {stop_ratio:.2f}...\")\n    avg_ratio = run_numpy_simulation(num_simulations, max_flips, stop_ratio)\n    results.append({'Stop Ratio': round(stop_ratio, 2), 'Expected Payout': avg_ratio})\n    print(\"=\"*50)\n\nprint(\"Simulations complete.\")\n# Results Dataframe! \ndf_all_results = pd.DataFrame(results).set_index('Stop Ratio')\n\n\ndf_all_results['Expected Value'] =  df_all_results['Expected Payout'] - 0.75  # Adjusted Expected Value\ndf_all_results['Expected Payout'].plot(\n    kind='line',\n    figsize=(10, 6),\n    marker='o',\n    title=f'Expected Payout across {num_simulations:,} Simulations',\n    xlabel='Stop Ratio',\n    ylabel='Heads / Total Flips',\n    color ='green'\n)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_all_results\n\n\n\n\n\n\n\n\nExpected Payout\nExpected Value\n\n\nStop Ratio\n\n\n\n\n\n\n0.50\n0.785382\n0.035382\n\n\n0.51\n0.785749\n0.035749\n\n\n0.52\n0.786531\n0.036531\n\n\n0.53\n0.787559\n0.037559\n\n\n0.54\n0.788118\n0.038118\n\n\n0.55\n0.788536\n0.038536\n\n\n0.56\n0.788972\n0.038972\n\n\n0.57\n0.788434\n0.038434\n\n\n0.58\n0.788622\n0.038622\n\n\n0.59\n0.787903\n0.037903\n\n\n0.60\n0.787989\n0.037989\n\n\n0.61\n0.788028\n0.038028\n\n\n0.62\n0.786756\n0.036756\n\n\n0.63\n0.785774\n0.035774\n\n\n0.64\n0.784747\n0.034747\n\n\n\n\n\n\n\nFirst, we can confirm that these results are reasonable. Our ‘stop when heads &gt; tails’ strategy that we calculated earlier using pure mathematics is equivalent to a stop ratio of 0.5 heads / total flips. We can see that for 0.50 stop ratio our payout is modeled at about 0.7854, which is the exact value that we got when mathematically calculating its expected payout!\nFrom these results, it appears the optimal stopping ratio is around 0.56, yielding an expected payout of about 0.789 heads/total flips. This is approximately equal to a profit of 3.9 cents per game played! This is slightly better than our previous strategy of stopping when heads first exceeds tails. The 3.9 cent profit per game is now an ROI of about 5.2% on each 75 cent wager.\n\n\nThe Most Optimal Policy\nNow we have figured out the optimal policy for the ‘stop when heads/total flips &gt; X’ strategy. However, is there a better policy that could outperform stop when ‘heads/total flips &gt; X’?\nIn theory, yes. By creating some policy that scales stopping ratio by marginal variance per flip you could achieve better outcomes (think about this and try it out yourself). However, instead we will focus on a more theoretically interesting idea.\nBut first as an aside, let’s introduce potentially one of the coolest named theorems in probability: the Infinite Monkey Theorem (link).\n\n\n\nInfinite Monkey Theorem\n\n\n\nThe core idea of the infinite monkey theorem is a thought experiment. Let’s say you have one monkey and have him sit down and randomly hit keys on a typewriter for an infinite amount of time. Given infinite time, the monkey will certainly type out any given possible text, including the complete works of Shakespeare.\n\nWell, let’s follow this logic for our coin flip game. Given infinite time, we would expect any sequence of heads and tails to eventually occur (if we never stop flipping). Obviously, in practice Python cannot model infinite time. However, a new policy appears when we consider this theorem. What if we simply only stop flipping when we reach a ratio of .999 repeated heads/total flips? While this may feel like a cop-out answer, it is in theory the optimal policy given we have infinite time to continue flipping the coin and never stop. If we truly can flip forever, or even simulate in some way infinite time, we would eventually reach a ratio of .999 repeated heads/total flips! This would give us a payout of 99.99 repeated cents, and a profit of 24.99 repeated cents per game played! This would be a massive ROI of 33.33 % repeated."
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#findings",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#findings",
    "title": "Modeling a Coin Flip Game",
    "section": "Findings",
    "text": "Findings\nThis analysis reveals that a seemingly unfavorable coin-flip game can be transformed into a profitable venture through the strategic application of optionality. The core finding is that by having the freedom to stop at any point, a player can devise a policy that systematically beats the game’s apparent long-run expectation of a 50-cent payout.\n\nAnalytical Solution:\n\nWe first established a baseline profitable strategy: stopping the game the first time the number of heads exceeds the number of tails. Using combinatorics and the properties of Catalan numbers, we analytically derived the expected payout for this strategy to be exactly π/4 ≈ 0.7854. With a cost of 0.75, this yields a positive expected value of approximately 3.54 cents per game.\n\nNumerical Optimization:\n\nThrough large-scale Monte Carlo simulations (2 million runs), we refined this approach by testing various static stopping thresholds. The simulations confirmed our analytical result and identified a more optimal policy: stopping only when the ratio of heads to total flips exceeds 0.56. This optimized policy increases the expected payout to approximately 0.789, boosting the expected profit to 3.9 cents per game and achieving a return on investment of over 5%.\n\nThe Value of Optionality:\n\nThe essential insight is that the player’s ability to choose when to stop provides a significant edge. While the long-term probability of heads is 50%, the player can selectively end the game during periods of favorable short-term variance, thereby capturing a payout greater than the underlying average."
  },
  {
    "objectID": "articles/posts/coin-flip-game/modelCoinFlipGame.html#future-work",
    "href": "articles/posts/coin-flip-game/modelCoinFlipGame.html#future-work",
    "title": "Modeling a Coin Flip Game",
    "section": "Future Work",
    "text": "Future Work\nThe strategies explored in this article, while profitable, are based on a static stopping rule. This opens the door to several avenues for more advanced research and optimization.\n\nDynamic Stopping Policy:\n\nThe primary limitation of the current model is its use of a constant stopping ratio. A more sophisticated approach would be to implement a dynamic policy where the decision to stop or continue depends on the current state of the game (i.e., the number of heads and the total number of flips). For instance, a 52% ratio after 1,000 flips is far more statistically significant and secure than a 52% ratio after only 25 flips.\n\nAnalysis with a Biased Coin:\n\nThe analysis could be extended to scenarios involving a biased coin (where P(Heads) ≠ 0.5). This would require recalculating the expected values and likely shift the optimal stopping thresholds, providing insight into how the strategy adapts to different underlying probabilities.\n\nRisk-Adjusted Strategies:\n\nThe current work focuses exclusively on maximizing expected value. A real-world player might be risk-averse and prefer a strategy that, for example, maximizes the probability of breaking even or minimizes the chance of a significant loss. Future analysis could incorporate utility theory to develop strategies that align with different risk profiles.\n\nExploring Different Payout Structures:\n\nHow would the optimal strategy change if the casino altered the payout function? Investigating non-linear payouts (e.g., (heads/total_flips)^2) or introducing different cost structures would test the robustness of our findings and lead to more generalized solutions for this class of optionality-based games.\n\nThese are just some initial thoughts by me and I am open to discussion. As always, feel free to reach out to me for suggestions on future articles or comments."
  },
  {
    "objectID": "articles/posts/base-64-encoding/base_64_encoding.html",
    "href": "articles/posts/base-64-encoding/base_64_encoding.html",
    "title": "The Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "section": "",
    "text": "A journey from the 7-bit limitations of early dial-up to the multimodal prompts of today, exploring why ‘ASCII Armor’ remains the essential glue for modern AI."
  },
  {
    "objectID": "articles/posts/base-64-encoding/base_64_encoding.html#the-six-bit-bridge-why-modern-ai-still-speaks-in-1970s-ascii-armor",
    "href": "articles/posts/base-64-encoding/base_64_encoding.html#the-six-bit-bridge-why-modern-ai-still-speaks-in-1970s-ascii-armor",
    "title": "The Six-Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "section": "The Six-Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "text": "The Six-Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”\nIn the world of Generative AI, we often focus on the new developments: neural networks, transformer architectures, and trillion-parameter models. But if you peek under the hood of a request sent to GPT-5 or Gemini 3, you’ll find a relic from ages ago.\nIt looks like a wall of digital noise:\ndata:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...\nTo the uninitiated, this is gibberish. To a developer, it’s Base64. But to a computer historian, it goes by a more popular name: “ASCII Armor.”"
  },
  {
    "objectID": "articles/posts/base-64-encoding/base_64_encoding.html#the-missing-eighth-bit",
    "href": "articles/posts/base-64-encoding/base_64_encoding.html#the-missing-eighth-bit",
    "title": "The Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "section": "The Missing Eighth Bit",
    "text": "The Missing Eighth Bit\nIn the 1970s, the internet was a “fragile” place. Early networks were designed for 7-bit ASCII, perfect for English text, but catastrophic for 8-bit binary files.\nData transmission historically was expensive and slow. To save space, engineers used 7-bit ASCII for text representation. 7 bits can represent \\(2^7\\), or 128 different values (0 to 127). This was enough to represent the full english alphabet (A-Z, a-z), numbers (0-9), basic punctuation, and ‘control characters’ like Enter or Tab.\nHowever, most computers at the time typically used 8-bit (1 byte) systems. If you tried to send a raw image, the network would often “strip” the eighth bit of every byte to save space. It was like removing every third letter from a sentence; the data arrived shredded. Worse, binary sequences could accidentally trigger “control codes”, which were bits of data that told a printer to stop or a modem to hang up.\nAs a solution, developers began “armoring” their data. By disguising binary files as a safe string of alphanumeric text, they ensured the data could survive the journey across imperfect networks. And thus, Base64 was created."
  },
  {
    "objectID": "articles/posts/base-64-encoding/base_64_encoding.html#what-is-base64",
    "href": "articles/posts/base-64-encoding/base_64_encoding.html#what-is-base64",
    "title": "The Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "section": "What is Base64?",
    "text": "What is Base64?\nTo solve the problem of sending raw data across the internet via traditional methods Base64 creates a mapping between the world of bits and bytes and the world of text and symbols.\nHere is how it works. Lets first take 3 bytes of information, containing 24 individual bits. With these 24 bits we will then split them into 4 groups of 6 bits. For each of the 6-bit groups we will map them to one real char (shown below in the table).\n\n\n\nValue Range\nBinary Range\nCharacters\n\n\n\n\n0–25\n000000–011001\nA–Z\n\n\n26–51\n011010–110011\na–z\n\n\n52–61\n110100–111101\n0–9\n\n\n62\n111110\n+\n\n\n63\n111111\n/\n\n\n\nThis does come at a cost however. For each 3 bytes of information, you will notice that this process created 4 characters. You may ask, well why don’t we just use more characters to the point where we can represent each byte with a single character.\nThis is a great question, the key problem is that representing each number in a fully byte requires \\(2^8\\) or 256 different values. We do not have that many ‘clean’ char values that are supported over standard text mechanisms."
  },
  {
    "objectID": "articles/posts/base-64-encoding/base_64_encoding.html#what-does-base64-have-to-do-with-llms-and-modern-ai",
    "href": "articles/posts/base-64-encoding/base_64_encoding.html#what-does-base64-have-to-do-with-llms-and-modern-ai",
    "title": "The Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "section": "What does Base64 have to do with LLMs and modern AI?",
    "text": "What does Base64 have to do with LLMs and modern AI?\nWell, these days we sure do love API’s. For the uninitiated, API stands for application program interface. With API’s we can abstractly send a request to a server and receive a desired response. Modern API developers love a format called JSON. This format is very similar (often times interchangeable) with a generic nested form of Python dictionaries and lists.\n\njson_obj = {\n  \"key1\": \"value1\",\n  \"key2\": 123,\n  \"key3\": true,\n  \"key4\": [\n    \"item1\",\n    \"item2\"\n  ],\n  \"key5\": {\n    \"nestedKey\": \"nestedValue\"\n  }\n}\nHowever when sending JSON requests, it becomes very hard to include raw data in the form of a file. And even if you could just include raw bytes, this represents a serious security issue. This would be allowing a random API user to execute arbitrary bytes of data on your computer or server. Very bad stuff. However thankfully instead we can turn to Base64 and old fashioned ASCII Armor to fix this problem."
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html",
    "href": "projects/6CardGolf/IMPROVEMENTS.html",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Issue: flippedInitialCards, flippedOpponentInitialCards, flippedHostInitialCards were used but not initialized in gameState\nFix: Added these variables to the gameState object and properly reset them in resetGameState()\n\n\n\n\n\nIssue: Discard pile was sometimes treated as an array, sometimes as a single card\nFix: Ensured discard pile is always treated as an array with proper null checks\n\n\n\n\n\nIssue: Timing issues during the initial card flipping phase could cause desynchronization\nFix: Added better logging and validation to track flip progress and prevent race conditions\n\n\n\n\n\nIssue: WebRTC connection failures and message parsing errors weren’t properly handled\nFix: Added comprehensive error handling with user-friendly messages\n\n\n\n\n\nIssue: Cards could become unclickable unexpectedly due to improper state management\nFix: Improved button state management and added validation checks\n\n\n\n\n\n\n\n\nAdded timeout mechanism for ICE gathering (10 seconds)\nImplemented heartbeat mechanism to detect connection issues\nAdded better error recovery and user feedback\nImproved connection state monitoring\n\n\n\n\n\nAdded validateGameState() function to catch inconsistencies early\nIntegrated validation into UI updates\nAdded safety checks for array operations\n\n\n\n\n\nAdded null checks for all array operations\nImproved error messages with context\nAdded system chat messages for connection issues\n\n\n\n\n\nAdded more responsive CSS for mobile devices\nImproved button state management\nBetter visual feedback for connection status\nAdded logging for debugging\n\n\n\n\n\nAdded comprehensive logging for debugging\nImproved function documentation\nBetter separation of concerns\nMore robust state management\n\n\n\n\n\n\n\n\nAutomatically detects connection issues\nSends periodic heartbeat messages every 30 seconds\nProvides early warning of connection problems\n\n\n\n\n\nBetter responsive design for small screens\nImproved touch targets and spacing\nOptimized layout for mobile devices\n\n\n\n\n\nAdded validation function to catch state inconsistencies\nEnhanced logging throughout the application\nBetter error reporting to users\n\n\n\n\n\n\n\n\nProper cleanup of intervals and event listeners\nBetter resource management for WebRTC connections\n\n\n\n\n\nReduced unnecessary UI updates\nBetter state synchronization\nImproved message handling efficiency\n\n\n\n\n\nBetter input validation\nSanitized message handling\nProtected against malformed data\n\n\n\n\n\n\nConnection Testing: Test with various network conditions\nMobile Testing: Verify responsive design on different screen sizes\nEdge Cases: Test with slow connections and connection drops\nGame Logic: Verify all game rules work correctly\nError Recovery: Test recovery from various error conditions\n\n\n\n\n\nReconnection Logic: Automatic reconnection when connection is lost\nGame History: Save and display game history\nSound Effects: Add audio feedback for game actions\nAnimations: Smooth transitions between game states\nAccessibility: Improve accessibility features\nOffline Mode: Local game mode for practice\n\n\n\n\n\nwebrtc.js: WebRTC connection improvements and heartbeat mechanism\n6cardgolf.js: Game logic fixes and validation\n6cardgolfUI.js: UI improvements and better state management\nstyle.css: Enhanced responsive design\nindex.html: No changes needed\n\nAll improvements maintain backward compatibility and don’t break existing functionality."
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#critical-bugs-fixed",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#critical-bugs-fixed",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Issue: flippedInitialCards, flippedOpponentInitialCards, flippedHostInitialCards were used but not initialized in gameState\nFix: Added these variables to the gameState object and properly reset them in resetGameState()\n\n\n\n\n\nIssue: Discard pile was sometimes treated as an array, sometimes as a single card\nFix: Ensured discard pile is always treated as an array with proper null checks\n\n\n\n\n\nIssue: Timing issues during the initial card flipping phase could cause desynchronization\nFix: Added better logging and validation to track flip progress and prevent race conditions\n\n\n\n\n\nIssue: WebRTC connection failures and message parsing errors weren’t properly handled\nFix: Added comprehensive error handling with user-friendly messages\n\n\n\n\n\nIssue: Cards could become unclickable unexpectedly due to improper state management\nFix: Improved button state management and added validation checks"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#improvements-made",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#improvements-made",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Added timeout mechanism for ICE gathering (10 seconds)\nImplemented heartbeat mechanism to detect connection issues\nAdded better error recovery and user feedback\nImproved connection state monitoring\n\n\n\n\n\nAdded validateGameState() function to catch inconsistencies early\nIntegrated validation into UI updates\nAdded safety checks for array operations\n\n\n\n\n\nAdded null checks for all array operations\nImproved error messages with context\nAdded system chat messages for connection issues\n\n\n\n\n\nAdded more responsive CSS for mobile devices\nImproved button state management\nBetter visual feedback for connection status\nAdded logging for debugging\n\n\n\n\n\nAdded comprehensive logging for debugging\nImproved function documentation\nBetter separation of concerns\nMore robust state management"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#new-features-added",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#new-features-added",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Automatically detects connection issues\nSends periodic heartbeat messages every 30 seconds\nProvides early warning of connection problems\n\n\n\n\n\nBetter responsive design for small screens\nImproved touch targets and spacing\nOptimized layout for mobile devices\n\n\n\n\n\nAdded validation function to catch state inconsistencies\nEnhanced logging throughout the application\nBetter error reporting to users"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#technical-improvements",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#technical-improvements",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Proper cleanup of intervals and event listeners\nBetter resource management for WebRTC connections\n\n\n\n\n\nReduced unnecessary UI updates\nBetter state synchronization\nImproved message handling efficiency\n\n\n\n\n\nBetter input validation\nSanitized message handling\nProtected against malformed data"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#testing-recommendations",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#testing-recommendations",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Connection Testing: Test with various network conditions\nMobile Testing: Verify responsive design on different screen sizes\nEdge Cases: Test with slow connections and connection drops\nGame Logic: Verify all game rules work correctly\nError Recovery: Test recovery from various error conditions"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#future-enhancements",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#future-enhancements",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "Reconnection Logic: Automatic reconnection when connection is lost\nGame History: Save and display game history\nSound Effects: Add audio feedback for game actions\nAnimations: Smooth transitions between game states\nAccessibility: Improve accessibility features\nOffline Mode: Local game mode for practice"
  },
  {
    "objectID": "projects/6CardGolf/IMPROVEMENTS.html#files-modified",
    "href": "projects/6CardGolf/IMPROVEMENTS.html#files-modified",
    "title": "6 Card Golf Game - Bug Fixes and Improvements",
    "section": "",
    "text": "webrtc.js: WebRTC connection improvements and heartbeat mechanism\n6cardgolf.js: Game logic fixes and validation\n6cardgolfUI.js: UI improvements and better state management\nstyle.css: Enhanced responsive design\nindex.html: No changes needed\n\nAll improvements maintain backward compatibility and don’t break existing functionality."
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "6 Card Golf \n       P2P 1v1 card game with a chat. \n    \n  \n\n  \n    \n       \n      \n       Dig Dug Game \n      A classic arcade game recreation."
  },
  {
    "objectID": "articles/posts/mortgage-from-market/mortgage-from-market.html",
    "href": "articles/posts/mortgage-from-market/mortgage-from-market.html",
    "title": "Getting a Mortgage from the Market",
    "section": "",
    "text": "Articles showcase a select group using options to borrow money rather than a traditional Agency mortgage. Are advertisements of $0 down and near treasury-rate mortgages a fact or fiction?\nImagine securing a loan for your dream home with a 0% down payment and an interest rate that rivals the U.S. Treasury. It sounds like a financial fantasy, but a niche corner of the options market, the ‘box spread’, has been touted online as a secret Wall Street trick to do just that. But is this a golden ticket for the average homebuyer, or a high-stakes gamble reserved for a select few? Let’s break down the math, the hype, and the hidden dangers.\nToday the average 30-year fixed rate mortgage is around 6.3% yearly. Generally for mortgages it is recommended that buyers have around 20% to put as a ‘down payment’ at the start of the mortgage term. When trying to purchase a $1M house the math looks like the following:\nHome cost:  $1M  Down payment:  20% * $1M = $200K  Loan value:  $1M - $200K = $800K  Interest rate monthly:  6.3%/12 = .525% monthly  Length of payments in months:  30 years * 12 = 360 months  Monthly payment:  $800K * .525% / (1-(1 + .525%)^-360) = $4,951.78  Total interest paid:  $4,951.78 * 360 - $800K = $982,640.8\nThere are good and bad things associated with this:\n[good]\n[bad]"
  },
  {
    "objectID": "articles/posts/mortgage-from-market/mortgage-from-market.html#the-box-spread-mortgage",
    "href": "articles/posts/mortgage-from-market/mortgage-from-market.html#the-box-spread-mortgage",
    "title": "Getting a Mortgage from the Market",
    "section": "The Box Spread Mortgage",
    "text": "The Box Spread Mortgage\nFirst let’s clarify, I am not suggesting you do this. In fact, today I am going to argue why this is not the Goldilocks scenario it is documented as in articles online. Please do not take this article as advice to go out and try and trade millions of dollars of notional volume of options to try and create yourself one of these.\nNow that that is out of the way, here is the box spread mortgage:\n\nA box-spread mortgage is a synthetic borrowing mechanism in which a long-dated options box spread provides capital upfront in exchange for a contractually fixed repayment at maturity, effectively replicating a mortgage’s economic profile via derivative pricing.\n\n\nCreating a Box Spread Mortgage\nSimply put to create a box spread we go long a call and short a put option at a strike \\(k_1\\) and short a call and long a put at strike \\(k_2\\). At strike \\(k_1\\) you have effectively created a synthetic long forward. At strike \\(k_2\\) you have created a short forward.\nIt is important that the options that comprise this trade are European options. American options allow for exercising an option early, allowing for your ‘balanced hedge’ to fall apart whenever your counterparty decides.\nGoing long a forward is equivalent to agreeing to buy an asset at a fixed-price \\(k_1\\) in the future. Going short a forward is agreeing to sell an asset at a fixed-price \\(k_2\\) in the future.\nThus, when \\(k_2\\) &gt; \\(k_1\\) we receive a fixed future payoff of \\(k_2 - k_1\\). We also collect upfront the present value of this payoff discounted at the risk-free rate (or 10s of bps higher as market makers need to profit) this upfront cash flow is \\(PV(k_2 - k_1)\\).\nSo in this trade we get paid \\(PV(k_2 - k_1)\\) and have to pay \\(k_2 - k_1\\) in the future. This is the exact same as borrowing \\(k_2 - k_1\\) dollars at/near the risk-free interest rate \\(r_f\\).\nIn simpler terms you can think of it this way: you simultaneously make two deals for a future date.\n\nDeal 1: You agree to buy an asset for $1,000.\nDeal 2: You agree to sell the exact same asset for $1,050.\n\nNo matter what the asset’s price is on that future date, you are guaranteed to make a $50 profit. The market recognizes this guaranteed future profit and gives you its present value today: say, $48 as cash in your pocket.\nCongratulations, you’ve just received $48 upfront in exchange for paying back $50 later. In essence, you’ve taken out a loan at a very low interest rate, with the loan amount being the present value of the spread between your two deals.\n\n\nHow Much?\nJust how far from \\(r_f\\) can you borrow? Due to recent increase in volume in the SPX box spread market, this CBOE article (which I recommend reading if you are interested in this) says to expect 30-50bps higher than treasury rates! This is astonishingly low with the 5yr sitting at 3.6% on the high end you could expect to borrow at 4.1% interest! This is 2.2% lower than the traditional mortgage I showed earlier.\nNot to mention there are other benefits as well. I mentioned that you get paid upfront whatever you want, let’s say the full 1M value of the home. In 5 years you repay \\(\\$1M * (1.041)^5 = \\$1,222,513.45\\). Costing you ~200K to loan 1M over 5 years. At the comparable interest rate for the traditional mortgage this would cost around \\(\\$1M * (1.063^5) - \\$1M = \\$357k\\). No monthly payments as well with the box spread, just the lump sum payment at a future date of over a million dollars.\n\n\nAll the Problems\nOk first off in order to actually get one of these box loans your strategy (as a retail trader) will need to be fully collateralized at all times. This means if you are to do the $1M loan you need $1.2M+ in assets in your same brokerage account at all times. This is ok for some people however it requires a lot of capital depending on how volatile a person’s investments are.\nI would also guess that there is a correlation between using a box spread as a mortgage and having a higher risk appetite, but I digress.\nIn a bad market downturn the stock market can fall ~50% so to be safe let’s say we will need to have $2.4M in equities or less if you are including less volatile fixed-income products as well. Immediately we have moved from ‘easily accessible risk-free rate mortgage’ to ‘you need double or more in retirement assets to reasonably attempt this’.\nLet’s say we are fully collateralized, we are very certain we will be able to meet the liability in 5 years safely. Now we have another problem, we have to meet a 5 year liability of greater than our loan size settled in pure cash! If you were worried about selling 20% of your home value in portfolio value how about selling 120% of your home value in portfolio value in just 5 years!\nNow this is not fully fair as there are likely strategies to ‘kick the cash flow down the road’ like rolling into a new 5 year box spread borrowing the new amount (collateralized fully by at least a 2x multiple). However, it’s still said that at some point you need to present a large quantity of cash in order to settle this contract. The only way to come up with that level of cash is by selling assets or making incremental payments to set aside for the large future negative cash flow you expect. This is why in a traditional mortgage you are expected to pay down a fixed cost monthly which goes to both repaying interest and the principal which you loaned out.\nIn order to effectively execute this strategy you need to be very wealthy in retirement assets which you can use as collateral covering the loan size multiple times over if you have a more volatile portfolio. You also need to have a strategy to meet the expected large negative cash flow in the future.\nIf in 5 years, you need to have some plan to come up with $1.2M then rather than just coming up with the $1M today? Given you are this consumer who has vast riches in assets and is not concerned with the future cash liability why not just pay off the house today? Pushing the house/sale of retirement assets out 5 years even at the risk-free rate does not seem as worth it for this consumer since they already have more than enough money for the loan anyway.\nWell, there are some reasons. Here are the reasons why you would want to do this. First, the interest payments show up as a loss in an options trade. This loss is tax deductible! This means if you are wealthy you can likely write off a large portion of the interest (leading to an almost free loan). So let’s say you are fully collateralized with an investment account however you expect to have to sell off some assets in 5 years that have appreciated (incurring a capital gains tax). You also happen to be making a large purchase that you could have done outright anyway. Now we have a great scenario for you to go out to the market, borrow money using box spreads, and then generate a loss at the end of the period allowing you to:\n\nBorrow money today so that you don’t have to sell assets.\nAlign your planned sale of assets (likely with capital gains tax) and offset it with the loss you will generate in the synthetic loan strategy (interest payment).\nPay off the full cost of the loan at the end of the period rather than the start.\n\nThis now looks like a great trade.\nOnce again though I want to reiterate, this is not a good strategy for people who are under-collateralized. It is in fact an awful strategy. If the market drops and your collateral falls bellow whatever threshold your broker allows you wont be losing some fancy new home, you will be losing your life savings (and all investments). This is not a good trade for that level of risk.\n\n\nConclusion\nThere is a select group who could benefit from the box spread mortgage (or loan).\nThis select group is over-collateralized, has planned selling with tax implications in the future, and wants to make a purchase today. This group is not the everyday person, in fact far from it! Most people get a mortgage because they want to convert the large upfront cost burden of owning a home to a delayed fixed cost burden spread over many years.\nThis consumer does not have enough to buy the house outright and likely does not have enough to buy several of the house within their retirement assets. Thus, if they tried the box spread mortgage they would likely get margin called during a large drawdown in the equity markets and lose everything.\nAs always, till next time.\nJCP"
  },
  {
    "objectID": "articles/posts/deepseek-ocr/deepseek-ocr.html",
    "href": "articles/posts/deepseek-ocr/deepseek-ocr.html",
    "title": "The Future of Context, DeepSeek OCR",
    "section": "",
    "text": "A breakthrough in optical character recognition (OCR) has profound implications for the future of LLMs.\nLarge language models are usually limited by how much text they can fit into their context windows. Every token matters. This is especially true when processing long PDFs, research papers, or codebases. But a recent breakthrough in optical character recognition (OCR) suggests something surprising: vision tokens may soon become a dramatically more efficient way to feed information to LLMs than text tokens.\nIn this article, I’ll explain what OCR actually is, how the technology evolved, what changed recently, and why this discovery might reshape the future of context."
  },
  {
    "objectID": "articles/posts/deepseek-ocr/deepseek-ocr.html#what-the-hell-is-an-ocr",
    "href": "articles/posts/deepseek-ocr/deepseek-ocr.html#what-the-hell-is-an-ocr",
    "title": "The Future of Context, DeepSeek OCR",
    "section": "What the hell is an OCR?",
    "text": "What the hell is an OCR?\nOCR stands for Optical Character Recognition. It is the core technology that allows computers to convert real world documents, such as pdfs, images, and scanned paper documents, into machine-readable text. In other words, OCR transforms a messy visual input into a structured text representation that can be fed into other processes like LLMs.\n\nA Brief History of OCR\nOCR technology is not new, its history actually dates back to the 20th century. In the 1910s, Dr. Edmund Fournier d’Albe of Birmingham University developed the first ever OCR system, named the Optophone.\n\n\n\nOptophone machine\n\n\nThe Optophone was able to scan printed text and convert it into distinct sounds allowing for a blind user to recognize what characters were being scanned. The system was rule-based and relied on the physical height and width of characters to identify them.\n\n\n\nOptophone Methodology\n\n\nNot much of significance happened with the technology until the 1950s, when developments in digital computers and light sensors allowed OCR to take off. OCR was no longer physical rules based systems and now involved using computers to run similarity algorithms evaluating which letters were present. IBM and RCA began to develop OCR technology for all sorts of tasks. IBM created the first commercially available scanner that could read handwritten text.\nIn the ’60s and ’70s, OCR became a key software for everything from the postal service sorting mail to banks processing checks (see MICR). Even to this day OCR software is used all around the world in a vast array of commercial applications.\nModern developments in the 21st century involving neural networks have offered even more room for OCR improvement, as we will see with DeepSeekOCR."
  },
  {
    "objectID": "articles/posts/deepseek-ocr/deepseek-ocr.html#deepseek-ocr",
    "href": "articles/posts/deepseek-ocr/deepseek-ocr.html#deepseek-ocr",
    "title": "The Future of Context, DeepSeek OCR",
    "section": "DeepSeek OCR",
    "text": "DeepSeek OCR\nEarlier this month, a Chinese hedge fund’s AI lab, DeepSeek released a paper outlining a new OCR model. Without even reading the paper, the initial community reaction was something along the lines of:\n\nDeepSeek continues to enter a new market (OCR) and is killing it again (claiming 97%+ accuracy on general OCR benchmarks)\n\nOCR software is old, boring software that just turns writing/images into well formatted text and has many commercial applications for DeepSeek to monetize\n\nI would say the initial read and public reception of this work is 100% correct. However, it misses the whole point of the paper! There is one core aspect which outlines why DeepSeek went through the trouble to make yet another OCR model.\n\nIt’s always been about context!\nIn the world of machine learning and natural language processing, context is king. The ability to understand and utilize context is what allows models to generate coherent and relevant responses.\nAnyone who uses AI assistants learns quite quickly about the importance of context. Nothing is worse than chatting with the model for 10 minutes then having to start all over explaining a topic when you hit the context limit for the model. Eventually, you are forced to open a new ChatGPT or Claude tab and start the whole conversation all over again.\nHere is the one fascinating thing I have not yet told you about the brand new DeepSeek OCR model: vision tokens are dramatically more information-dense than text tokens when representing documents.\nNow this is an interesting development! Currently the largest LLM with strong performance and long context is the Gemini model, claiming 2 million tokens of context. Each token is approximately \\(\\frac{1}{4}\\) of a word. Given around 500 words per page, 2 million tokens of context gets you about 3,000 pages of context or about 1,500,000 words.\nIn this paper, DeepSeek is stating that they can compress text tokens 10x when using them as vision tokens with very high accuracy. If this is true you could extend Gemini’s context window from 2 million tokens to 20 million. 1.5 million words versus 15 million words, 3,000 pages versus 30,000 pages. This is like going from being able to see 1 volume of the Encyclopedia Britannica versus 12 volumes (the full 32 volumes are 50 Million words).\n\n\nWhat’s the catch?\nWell, I did say if this is true and currently it is not. Some language models understand vision tokens at high fidelity; however, this fidelity is not nearly on par with typical text. In order to achieve a full 10x of the context window the model would have to understand vision tokens just as well as it understands text tokens without decoding them back into text tokens (removing any compression benefit). Text has semantic meaning, typically previous words in a sentence have strong correlation with the future words in that same sentence. Vision tokens share this relationship but on a much smaller scale. Since the correlation to future tokens is so small, this leads to a much worse ability to generate coherent output.\nThere have been very interesting workarounds though and research in the space is ongoing. One pervasive thought in the industry right now is the idea of storing context in cached vision tokens, but only converting necessary context into text tokens for the model to actively use. This seems to be a plausible and reasonable future development of vision token compression.\n\n\nWell, let’s test it out!\nLast week the following was trending on both Twitter and Linkedin in relation to the new DeepSeek-OCR model.\n\n\n\nDeepSeek-OCR on Ramanujan’s letter to GH Hardy 1913 (could not find any citation for this)\n\n\nThis looks shocking; the accuracy level seems to be very high even on complex handwritten mathematical formulas from over 100 years ago! This level of accuracy prompted (awful pun) a healthy level of skepticism. Let’s test this ourselves and see how accurate it is on both the Ramanujan letter as well as some brand new handwriting of my own.\n\nDeepSeek-OCR result on letter to GH Hardy 1913\nThe following is the letter after being passed through DeepSeek-OCR at the highest compute setting, I used Google Colab with an A100 for this:\n——————————– start ——————————–\nDear Mr Hardy,\nIn one of my letters I wrote about the least number of terms which will give the mean est integral to the actual coefficient in \\(\\frac{1}{2}\\) problem. It will be actually difficult to prove such a result. But we can prove this much as follows.\nequation:\n\\[\n\\begin{align*}\n\\sum a_n x^n &= \\frac{1}{1-50(1-\\frac{1}{2}x+\\frac{1}{1-2x}+\\cdots)} \\\\\na_n &= c \\left[ e^{2n\\pi} + (-1)^n \\frac{e^{n\\pi}}{2^5} + 2 \\cos\\left(\\frac{2n\\pi}{5} + 8(2n-1)\\frac{e^{\\frac{2n\\pi}{5}}}{5} + \\cdots\\right) + 2 \\cos\\left(\\frac{2n\\pi}{5} + 8(3n-1)\\frac{e^{\\frac{2n\\pi}{5}}}{5} + \\frac{2n\\pi}{5} + \\cdots\\right) \\right]\n\\end{align*}\n\\]\nequation:\n\\[\n\\begin{align*}\nc = \\frac{3}{2} \\cdot \\frac{[\\Gamma(\\frac{3}{2})]^6}{\\pi^6} = .94373...\n\\end{align*}\n\\]\ntext:\nWe shall first prove that, if we take only\n\\[\n\\begin{align*}\n\\left[ \\frac{3}{2} n(1-c) \\right]^{\\frac{1}{2}} \\sqrt{(1-\\frac{1}{2}n)(1-\\frac{1}{4}n)(1-\\frac{1}{8}n)\\cdots}\n\\end{align*}\n\\]\ntext:\nterms in the right-hand side of (1), E being any positive number less than 1 and 5, 13, 17, … are primes of the form \\(4k+1\\), then it is possible to find an infinity of values of \\(n\\) for which \\(dn\\) is not the mean of in- tegers to the sum of the asymptote series up to (2) terms. Suppose that \\(a_n\\) is the number of terms of any two squares such as 1, 2, 4, 5, 9, 10, 13, 16, … not exceeding \\(\\lambda\\) and that \\(p(\\lambda)\\) is the number of sums of two squares that are prime to each other such as 1, 2, 5, 10, 13, 17, 25 … not exceeding \\(\\lambda\\). Then it is easy to see that\n\nSpecial thank you to Trinity College for hosting this letter\n\n——————————– end ——————————–\nWow, that is some shocking levels of accuracy. This level of accuracy on handwritten text seems insane. Just to confirm this is not already seen in the training data of the model I will now run the model but with some awful handwriting of my own. This is a handwritten page of homework from college. Similar to the page above it contains a lot of scribbled math which would be hard for even me to decipher at times.\nHere is the homework page:\n\n\n\nOld math homework\n\n\nHere is the model output:\n——————————– start ——————————–\ntext:\nmiddle term is zero. Since \\(E_{\\epsilon}[\\epsilon] = 0\\)\nequation:\n\\[\n\\begin{align*}\nE_x[(g_D - s(x))^{2}] + \\sigma^{2}, \\qquad\nE_D!\\left[(g_D - s(x))^{2}\\right]\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n= \\left(E_D[g_D] - s(x)\\right)^{2} + \\mathrm{Var}_D(g_D) \\\nE\\left[E_D(g_D)\\right]\n= E_x!\\left[\\text{bias}^{2} + \\text{variance}\\right] + \\sigma^{2} \\\n= \\text{bias}^{2} + \\text{variance} + \\sigma^{2}\n\\end{align*}\n\\]\ntext:\n\\[\nE\\left[E_D(g_D)\\right]\n= E_x!\\left[\\text{bias}^{2} + \\text{variance}\\right] + \\sigma^{1}\n= \\text{bias}^{2} + \\text{variance} + \\sigma^{1}\n\\]\ntext:\n\n\n\nequation:\n\\[\n[\nD = {(x_1, x_1^2), (x_2, x_2^2)}, \\qquad\nP = 1 = \\mathrm{Dim}(D), \\qquad\ns(x) = x^{2}\n]\n\\]\ntext:\nunion over \\([-1, 1]\\), \\(H = \\mathrm{th}(h(x)) = ax + b\\) for some \\(a,b \\in \\mathbb{R}\\) and we wish to minimize Square Error!\ntext:\n\n\\(a x_1 + b = x_1^2,\\quad a x_2 + b = x_2^2\\)\n\nequation:\n\\[\n\\begin{align*}\na x_1 + b - x_1^{2} &= a x_2 + b - x_2^{2} \\\na x_1 - x_1^{2} &= a x_2 - x_2^{2} \\\na(x_1 - x_2) &= x_1^{2} - x_2^{2} \\\na(x_1 - x_2) &= (x_1 + x_2)(x_1 - x_2) \\\na &= x_1 + x_2\n\\end{align*}\n\\]\nequation:\n\\[\n\\begin{align*}\n(x_1 + x_2)x_1 + b &= x_1^{2} \\\nx_1^{2} + x_1 x_2 + b &= x_2^{2} \\\nb &= -x_1 x_2\n\\end{align*}\n\\]\nequation:\n\\[\n[\n\\tilde{g}(x)\n= E[h_0(x)]\n= E[(x_1 + x_2)x - x_1 x_2]\n= 0\n]\n\\]\ntext:\n\nTo numerically estimate \\(g(x)\\), \\(E_D(x)\\), bias, and variance, I have devised the following estimator. First, generate many training sets \\(g(x_1, x_2)\\) for values in uniform \\([-1,1]\\) space. Next we fit our hypothesis line to this data. The average of these hypothesis lines will be our estimate of \\(g(x)\\).\n\ntext:\nFor each hypothesis we also compute\n\\[\n[\nE_D(x) = \\frac{1}{n} \\sum_{i=1}^{n} (g(x_i) - x_i^{2})\n]\n\\]\ntext:\nFor every sample we solve this as follows: $ (x) = (g(x) - x)^2 $, where \\(g(x)\\) is always assumed.\ntext:\nOur variance estimator will be:\n\\[\n[\n\\mathrm{Var}(x)\n= \\frac{1}{n} \\sum_{i=1}^{n} \\left(g(x_i) - \\tilde{g}(x_i)\\right)^{2}.\n]\n\\]\n——————————– end ——————————–\nFor sure a lot more errors in this one in terms of OCR quality. On my handwriting the model frequently messes up what the power was of variables, exact forms of equations, confuses vectors, and more. Given this performance I would highly agree with the comments I have seen on social media that the letters from Ramanujan to GH Hardy were likely in the training set of the DeepSeekOCR model. Performance on my handwriting was still very strong, but clearly not to the caliber of a model that almost perfectly gets the scribbled handwriting from the Ramanujan letter.\n\n\n\nCurrent use cases\nNaively, as a recent graduate who frequently had to write assignments in LaTeX (a mathematical formatting language) which was often time consuming given most of the math was worked out on paper (or iPad) already, I see some great use cases.\n\nPass paper homework into DeepSeekOCR, convert to markdown.\nAsk LLM to convert markdown to LaTeX\nVoila! You have saved ~an hour of writing your already-written homework into LaTeX!\n\nRealistically this is not the true value of this model, but nonetheless a very interesting use case. The real value of this model is in converting large quantities of handwritten text into visual tokens as a medium (of high information density and low loss), instead of converting them directly into text tokens. If breakthroughs happen in how visual tokens can be used to train models directly, this paper will become seen as a seminal work in token density. Honestly, on a personal note, I hope there is more development in this space; the medium of visual tokens has much more to offer than asking SORA to generate you a video of Trump doing a backflip onto the White House lawn.\nAs always, till next time.\nJCP"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JCP",
    "section": "",
    "text": "jcp@home:~\n\n\n\n\n\n\n\n\n 🌌"
  },
  {
    "objectID": "articles/posts/base-64-encoding/base_64_encoding.html#the-bit-bridge-why-modern-ai-still-speaks-in-1970s-ascii-armor",
    "href": "articles/posts/base-64-encoding/base_64_encoding.html#the-bit-bridge-why-modern-ai-still-speaks-in-1970s-ascii-armor",
    "title": "The Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "section": "The Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”",
    "text": "The Bit Bridge: Why Modern AI Still Speaks in 1970s “ASCII Armor”\nIn the world of Generative AI, we often focus on the new developments: neural networks, transformer architectures, and trillion-parameter models. But if you peek under the hood of a request sent to GPT-5 or Gemini 3, you’ll find a relic from ages ago.\nIt looks like a wall of digital noise:\ndata:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...\nTo the uninitiated, this is gibberish. To a developer, it’s Base64. But to a computer historian, it goes by a more popular name: “ASCII Armor.”"
  }
]